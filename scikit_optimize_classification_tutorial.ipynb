{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cwcol\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale the data to between 0 & 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 1.0\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train/ 255\n",
    "X_test = X_test/ 255\n",
    "print(X_train.min(),X_train.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, 0, 4, 1, 4, 9, 5, 9], dtype=uint8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to Flatten the Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(60000,784)\n",
    "X_test = X_test.reshape(10000,784)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the y's to used with softmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "y_train = np_utils.to_categorical(y_train, 10)\n",
    "y_test = np_utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784,)\n"
     ]
    }
   ],
   "source": [
    "input_shape= X_train[0].shape\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking our result against a baseline\n",
    "A simple 2 layer neural net with hardly any hyperparameter tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the usual imports for a vanilla nueral net\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_layer (Dense)          (None, 16)                12560     \n",
      "_________________________________________________________________\n",
      "hidden_layer (Dense)         (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "output_layer (Dense)         (None, 10)                170       \n",
      "=================================================================\n",
      "Total params: 13,002\n",
      "Trainable params: 13,002\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model =Sequential()\n",
    "model.add(Dense(16, input_shape=input_shape, activation='relu',name = 'input_layer'))\n",
    "model.add(Dense(16, activation='relu', name=\"hidden_layer\"))\n",
    "\n",
    "model.add(Dense(10,activation='softmax',name=\"output_layer\"))\n",
    "model.compile(optimizer = 'adam', loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 51000 samples, validate on 9000 samples\n",
      "Epoch 1/3\n",
      "51000/51000 [==============================] - 3s 57us/step - loss: 0.7272 - acc: 0.7819 - val_loss: 0.3120 - val_acc: 0.9128\n",
      "Epoch 2/3\n",
      "51000/51000 [==============================] - 1s 20us/step - loss: 0.3146 - acc: 0.9099 - val_loss: 0.2512 - val_acc: 0.9271\n",
      "Epoch 3/3\n",
      "51000/51000 [==============================] - 1s 21us/step - loss: 0.2665 - acc: 0.9229 - val_loss: 0.2221 - val_acc: 0.9357\n"
     ]
    }
   ],
   "source": [
    "blackbox = model.fit(X_train, y_train, batch_size=128, epochs =3, validation_split=.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 26us/step\n",
      "0.9283\n"
     ]
    }
   ],
   "source": [
    "accuracy = model.evaluate(X_test,y_test)[1]\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Skopt (scikit-optimize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports we know we'll need\n",
    "import skopt\n",
    "# !pip install scikit-optimize if  necessary\n",
    "from skopt import gbrt_minimize, gp_minimize\n",
    "from skopt.utils import use_named_args\n",
    "from skopt.space import Real, Categorical, Integer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating our search parameters.\n",
    "\"dim_\" short for dimension. Its' just a way to label our parameters.\n",
    "\n",
    "We can search across nearly every param in a keras model. \n",
    "This code focuses on: \n",
    "* Number of Layers\n",
    "* Number of Nodes per layer\n",
    "* Learning Rate & Weight Decay for the Adam Optimizer\n",
    "* activation functions\n",
    "* batch size\n",
    "\n",
    "The name feature allows us to use the `@use_named_args` decorator.\n",
    "We must also establish default parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_learning_rate = Real(low=1e-4, high=1e-2, prior='log-uniform',\n",
    "                         name='learning_rate')\n",
    "dim_num_dense_layers = Integer(low=1, high=5, name='num_dense_layers')\n",
    "dim_num_input_nodes = Integer(low=1, high=512, name='num_input_nodes')\n",
    "dim_num_dense_nodes = Integer(low=1, high=28, name='num_dense_nodes')\n",
    "dim_activation = Categorical(categories=['relu', 'sigmoid'],\n",
    "                             name='activation')\n",
    "dim_batch_size = Integer(low=1, high=128, name='batch_size')\n",
    "dim_adam_decay = Real(low=1e-6,high=1e-2,name=\"adam_decay\")\n",
    "\n",
    "dimensions = [dim_learning_rate,\n",
    "              dim_num_dense_layers,\n",
    "              dim_num_input_nodes,\n",
    "              dim_num_dense_nodes,\n",
    "              dim_activation,\n",
    "              dim_batch_size,\n",
    "              dim_adam_decay\n",
    "             ]\n",
    "default_parameters = [1e-3, 1,512, 13, 'relu',64, 1e-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the usual imports for a vanilla nueral net\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import tensorflow\n",
    "from tensorflow.python.keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create our model which creates other models.\n",
    "We import the Adam optimizer to get the ability to adjust its learning rate and decay.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "def create_model(learning_rate, num_dense_layers,num_input_nodes,\n",
    "                 num_dense_nodes, activation, adam_decay):\n",
    "    #start the model making process and create our first layer\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_input_nodes, input_shape= input_shape, activation=activation\n",
    "                   ))\n",
    "    #create a loop making a new dense layer for the amount passed to this model.\n",
    "    #naming the layers helps avoid tensorflow error deep in the stack trace.\n",
    "    for i in range(num_dense_layers):\n",
    "        name = 'layer_dense_{0}'.format(i+1)\n",
    "        model.add(Dense(num_dense_nodes,\n",
    "                 activation=activation,\n",
    "                        name=name\n",
    "                 ))\n",
    "    #add our classification layer.\n",
    "    model.add(Dense(10,activation='softmax'))\n",
    "    \n",
    "    #setup our optimizer and compile\n",
    "    adam = Adam(lr=learning_rate, decay= adam_decay)\n",
    "    model.compile(optimizer=adam, loss='categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "@use_named_args(dimensions=dimensions)\n",
    "def fitness(learning_rate, num_dense_layers, num_input_nodes, \n",
    "            num_dense_nodes,activation, batch_size,adam_decay):\n",
    "\n",
    "    model = create_model(learning_rate=learning_rate,\n",
    "                         num_dense_layers=num_dense_layers,\n",
    "                         num_input_nodes=num_input_nodes,\n",
    "                         num_dense_nodes=num_dense_nodes,\n",
    "                         activation=activation,\n",
    "                         adam_decay=adam_decay\n",
    "                        )\n",
    "    \n",
    "\n",
    "    #named blackbox becuase it represents the structure\n",
    "    blackbox = model.fit(x=X_train,\n",
    "                        y=y_train,\n",
    "                        epochs=3,\n",
    "                        batch_size=batch_size,\n",
    "                        validation_split=0.15,\n",
    "                        )\n",
    "    #return the validation accuracy for the last epoch.\n",
    "    accuracy = blackbox.history['val_acc'][-1]\n",
    "\n",
    "    # Print the classification accuracy.\n",
    "    print()\n",
    "    print(\"Accuracy: {0:.2%}\".format(accuracy))\n",
    "    print()\n",
    "\n",
    "\n",
    "    # Delete the Keras model with these hyper-parameters from memory.\n",
    "    del model\n",
    "    \n",
    "    # Clear the Keras session, otherwise it will keep adding new\n",
    "    # models to the same TensorFlow graph each time we create\n",
    "    # a model with a different set of hyper-parameters.\n",
    "    K.clear_session()\n",
    "    tensorflow.reset_default_graph()\n",
    "    \n",
    "    return -accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It is always a good idea to run the code below everytime you restart the search function or the notebook\n",
    "\n",
    "The two lines of code below allows us to restart using tensorflow. Otherwise some stuff under the hood will cause problems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "tensorflow.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper parameters for our hyper parameter search\n",
    "For the Gaussian Project search, I have added a few extra parameters to try to improve the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 51000 samples, validate on 9000 samples\n",
      "Epoch 1/3\n",
      "51000/51000 [==============================] - 3s 51us/step - loss: 0.0021 - acc: 1.0000 - val_loss: 0.0745 - val_acc: 0.9819\n",
      "Epoch 2/3\n",
      "51000/51000 [==============================] - 2s 46us/step - loss: 0.0021 - acc: 1.0000 - val_loss: 0.0745 - val_acc: 0.9819\n",
      "Epoch 3/3\n",
      "51000/51000 [==============================] - 2s 46us/step - loss: 0.0021 - acc: 1.0000 - val_loss: 0.0749 - val_acc: 0.9819\n",
      "\n",
      "Accuracy: 98.19%\n",
      "\n",
      "Train on 51000 samples, validate on 9000 samples\n",
      "Epoch 1/3\n",
      "51000/51000 [==============================] - 3s 50us/step - loss: 0.0020 - acc: 1.0000 - val_loss: 0.0749 - val_acc: 0.9821\n",
      "Epoch 2/3\n",
      "51000/51000 [==============================] - 2s 47us/step - loss: 0.0020 - acc: 1.0000 - val_loss: 0.0744 - val_acc: 0.9822\n",
      "Epoch 3/3\n",
      "51000/51000 [==============================] - 2s 45us/step - loss: 0.0020 - acc: 1.0000 - val_loss: 0.0751 - val_acc: 0.9817\n",
      "\n",
      "Accuracy: 98.17%\n",
      "\n",
      "Train on 51000 samples, validate on 9000 samples\n",
      "Epoch 1/3\n",
      "51000/51000 [==============================] - 2s 36us/step - loss: 0.0019 - acc: 1.0000 - val_loss: 0.0748 - val_acc: 0.9820\n",
      "Epoch 2/3\n",
      "51000/51000 [==============================] - 2s 34us/step - loss: 0.0019 - acc: 1.0000 - val_loss: 0.0749 - val_acc: 0.9819\n",
      "Epoch 3/3\n",
      "51000/51000 [==============================] - 2s 33us/step - loss: 0.0019 - acc: 1.0000 - val_loss: 0.0753 - val_acc: 0.9818\n",
      "\n",
      "Accuracy: 98.18%\n",
      "\n",
      "Train on 51000 samples, validate on 9000 samples\n",
      "Epoch 1/3\n",
      "51000/51000 [==============================] - 2s 30us/step - loss: 0.0019 - acc: 1.0000 - val_loss: 0.0752 - val_acc: 0.9819\n",
      "Epoch 2/3\n",
      "51000/51000 [==============================] - 1s 28us/step - loss: 0.0019 - acc: 1.0000 - val_loss: 0.0749 - val_acc: 0.9821\n",
      "Epoch 3/3\n",
      "51000/51000 [==============================] - 1s 27us/step - loss: 0.0019 - acc: 1.0000 - val_loss: 0.0752 - val_acc: 0.9820\n",
      "\n",
      "Accuracy: 98.20%\n",
      "\n",
      "Train on 51000 samples, validate on 9000 samples\n",
      "Epoch 1/3\n",
      "51000/51000 [==============================] - 4s 80us/step - loss: 0.0019 - acc: 1.0000 - val_loss: 0.0752 - val_acc: 0.9824\n",
      "Epoch 2/3\n",
      "51000/51000 [==============================] - 4s 75us/step - loss: 0.0018 - acc: 1.0000 - val_loss: 0.0754 - val_acc: 0.9821\n",
      "Epoch 3/3\n",
      "51000/51000 [==============================] - 4s 74us/step - loss: 0.0018 - acc: 1.0000 - val_loss: 0.0758 - val_acc: 0.9819\n",
      "\n",
      "Accuracy: 98.19%\n",
      "\n",
      "Train on 51000 samples, validate on 9000 samples\n",
      "Epoch 1/3\n",
      "51000/51000 [==============================] - 68s 1ms/step - loss: 0.0019 - acc: 1.0000 - val_loss: 0.0756 - val_acc: 0.9823\n",
      "Epoch 2/3\n",
      "51000/51000 [==============================] - 68s 1ms/step - loss: 0.0018 - acc: 1.0000 - val_loss: 0.0764 - val_acc: 0.9817\n",
      "Epoch 3/3\n",
      "51000/51000 [==============================] - 68s 1ms/step - loss: 0.0017 - acc: 1.0000 - val_loss: 0.0765 - val_acc: 0.9823\n",
      "\n",
      "Accuracy: 98.23%\n",
      "\n",
      "Train on 51000 samples, validate on 9000 samples\n",
      "Epoch 1/3\n",
      "51000/51000 [==============================] - 1s 29us/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.0765 - val_acc: 0.9823\n",
      "Epoch 2/3\n",
      "51000/51000 [==============================] - 1s 27us/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.0766 - val_acc: 0.9821\n",
      "Epoch 3/3\n",
      "51000/51000 [==============================] - 1s 27us/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.0766 - val_acc: 0.9822\n",
      "\n",
      "Accuracy: 98.22%\n",
      "\n",
      "Train on 51000 samples, validate on 9000 samples\n",
      "Epoch 1/3\n",
      "51000/51000 [==============================] - 2s 32us/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.0766 - val_acc: 0.9823\n",
      "Epoch 2/3\n",
      "51000/51000 [==============================] - 1s 28us/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.0767 - val_acc: 0.9821\n",
      "Epoch 3/3\n",
      "51000/51000 [==============================] - 1s 28us/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.0767 - val_acc: 0.9822\n",
      "\n",
      "Accuracy: 98.22%\n",
      "\n",
      "Train on 51000 samples, validate on 9000 samples\n",
      "Epoch 1/3\n",
      "51000/51000 [==============================] - 2s 45us/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.0766 - val_acc: 0.9822\n",
      "Epoch 2/3\n",
      "51000/51000 [==============================] - 2s 42us/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.0767 - val_acc: 0.9818\n",
      "Epoch 3/3\n",
      "51000/51000 [==============================] - 2s 42us/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.0768 - val_acc: 0.9821\n",
      "\n",
      "Accuracy: 98.21%\n",
      "\n",
      "Train on 51000 samples, validate on 9000 samples\n",
      "Epoch 1/3\n",
      "51000/51000 [==============================] - 5s 106us/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.0769 - val_acc: 0.9821\n",
      "Epoch 2/3\n",
      "51000/51000 [==============================] - 5s 100us/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.0769 - val_acc: 0.9818\n",
      "Epoch 3/3\n",
      "51000/51000 [==============================] - 5s 101us/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.0770 - val_acc: 0.9820\n",
      "\n",
      "Accuracy: 98.20%\n",
      "\n",
      "Train on 51000 samples, validate on 9000 samples\n",
      "Epoch 1/3\n",
      "51000/51000 [==============================] - 2s 38us/step - loss: 0.0014 - acc: 1.0000 - val_loss: 0.0769 - val_acc: 0.9821\n",
      "Epoch 2/3\n",
      "51000/51000 [==============================] - 2s 37us/step - loss: 0.0014 - acc: 1.0000 - val_loss: 0.0768 - val_acc: 0.9821\n",
      "Epoch 3/3\n",
      "51000/51000 [==============================] - 2s 36us/step - loss: 0.0014 - acc: 1.0000 - val_loss: 0.0770 - val_acc: 0.9821\n",
      "\n",
      "Accuracy: 98.21%\n",
      "\n",
      "Train on 51000 samples, validate on 9000 samples\n",
      "Epoch 1/3\n",
      "51000/51000 [==============================] - 135s 3ms/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.0776 - val_acc: 0.9821\n",
      "Epoch 2/3\n",
      "51000/51000 [==============================] - 134s 3ms/step - loss: 0.0014 - acc: 1.0000 - val_loss: 0.0787 - val_acc: 0.9819\n",
      "Epoch 3/3\n",
      "51000/51000 [==============================] - 135s 3ms/step - loss: 0.0014 - acc: 1.0000 - val_loss: 0.0781 - val_acc: 0.9820\n",
      "\n",
      "Accuracy: 98.20%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gp_result = gp_minimize(func=fitness,\n",
    "                            dimensions=dimensions,\n",
    "                            n_calls=12,\n",
    "                            noise= 0.01,\n",
    "                            n_jobs=-1,\n",
    "                            kappa = 5,\n",
    "                            x0=default_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find our best accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best accuracy was 98.23%.\n"
     ]
    }
   ],
   "source": [
    "print(\"best accuracy was \" + str(round(gp_result.fun *-100,2))+\"%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### returning the parameters for the best function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.007141340424839902, 4, 7, 4, 'sigmoid', 2, 0.00537784574467934]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gp_result.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.98188889, -0.98166667, -0.98177778, -0.98199999, -0.98188889,\n",
       "       -0.98233333, -0.98222223, -0.98222223, -0.9821111 , -0.982     ,\n",
       "       -0.98211111, -0.982     ])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gp_result.func_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see all the models our search function tried"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>learning rate</th>\n",
       "      <th>hidden layers</th>\n",
       "      <th>input layer nodes</th>\n",
       "      <th>hidden layer nodes</th>\n",
       "      <th>activation function</th>\n",
       "      <th>batch size</th>\n",
       "      <th>adam learning rate decay</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001000</td>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>13</td>\n",
       "      <td>relu</td>\n",
       "      <td>64</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>98.188889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000218</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>8</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>63</td>\n",
       "      <td>0.000509</td>\n",
       "      <td>98.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001863</td>\n",
       "      <td>2</td>\n",
       "      <td>216</td>\n",
       "      <td>27</td>\n",
       "      <td>relu</td>\n",
       "      <td>92</td>\n",
       "      <td>0.001640</td>\n",
       "      <td>98.177778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001322</td>\n",
       "      <td>2</td>\n",
       "      <td>197</td>\n",
       "      <td>16</td>\n",
       "      <td>relu</td>\n",
       "      <td>112</td>\n",
       "      <td>0.001616</td>\n",
       "      <td>98.199999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000237</td>\n",
       "      <td>3</td>\n",
       "      <td>148</td>\n",
       "      <td>22</td>\n",
       "      <td>relu</td>\n",
       "      <td>38</td>\n",
       "      <td>0.001929</td>\n",
       "      <td>98.188889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.007141</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>2</td>\n",
       "      <td>0.005378</td>\n",
       "      <td>98.233333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.002599</td>\n",
       "      <td>3</td>\n",
       "      <td>115</td>\n",
       "      <td>3</td>\n",
       "      <td>relu</td>\n",
       "      <td>122</td>\n",
       "      <td>0.002137</td>\n",
       "      <td>98.222223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.009535</td>\n",
       "      <td>2</td>\n",
       "      <td>85</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>109</td>\n",
       "      <td>0.005409</td>\n",
       "      <td>98.222223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.001055</td>\n",
       "      <td>4</td>\n",
       "      <td>403</td>\n",
       "      <td>4</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>70</td>\n",
       "      <td>0.008651</td>\n",
       "      <td>98.211110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000165</td>\n",
       "      <td>2</td>\n",
       "      <td>218</td>\n",
       "      <td>26</td>\n",
       "      <td>relu</td>\n",
       "      <td>28</td>\n",
       "      <td>0.000883</td>\n",
       "      <td>98.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000545</td>\n",
       "      <td>2</td>\n",
       "      <td>196</td>\n",
       "      <td>14</td>\n",
       "      <td>relu</td>\n",
       "      <td>85</td>\n",
       "      <td>0.008431</td>\n",
       "      <td>98.211111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000100</td>\n",
       "      <td>5</td>\n",
       "      <td>512</td>\n",
       "      <td>28</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>98.200000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    learning rate  hidden layers  input layer nodes  hidden layer nodes  \\\n",
       "0        0.001000              1                512                  13   \n",
       "1        0.000218              2                 23                   8   \n",
       "2        0.001863              2                216                  27   \n",
       "3        0.001322              2                197                  16   \n",
       "4        0.000237              3                148                  22   \n",
       "5        0.007141              4                  7                   4   \n",
       "6        0.002599              3                115                   3   \n",
       "7        0.009535              2                 85                   2   \n",
       "8        0.001055              4                403                   4   \n",
       "9        0.000165              2                218                  26   \n",
       "10       0.000545              2                196                  14   \n",
       "11       0.000100              5                512                  28   \n",
       "\n",
       "   activation function  batch size  adam learning rate decay   accuracy  \n",
       "0                 relu          64                  0.001000  98.188889  \n",
       "1              sigmoid          63                  0.000509  98.166667  \n",
       "2                 relu          92                  0.001640  98.177778  \n",
       "3                 relu         112                  0.001616  98.199999  \n",
       "4                 relu          38                  0.001929  98.188889  \n",
       "5              sigmoid           2                  0.005378  98.233333  \n",
       "6                 relu         122                  0.002137  98.222223  \n",
       "7              sigmoid         109                  0.005409  98.222223  \n",
       "8              sigmoid          70                  0.008651  98.211110  \n",
       "9                 relu          28                  0.000883  98.200000  \n",
       "10                relu          85                  0.008431  98.211111  \n",
       "11             sigmoid           1                  0.010000  98.200000  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.concat([pd.DataFrame(gp_result.x_iters, columns = [\"learning rate\",\"hidden layers\",\"input layer nodes\",\"hidden layer nodes\",\n",
    "                                           \"activation function\",\"batch size\",\"adam learning rate decay\"]),\n",
    "(pd.Series(gp_result.func_vals*-100, name=\"accuracy\"))], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.007141340424839902, 4, 7, 4, 'sigmoid', 2, 0.00537784574467934]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gp_result.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 7)                 5495      \n",
      "_________________________________________________________________\n",
      "layer_dense_1 (Dense)        (None, 4)                 32        \n",
      "_________________________________________________________________\n",
      "layer_dense_2 (Dense)        (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "layer_dense_3 (Dense)        (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "layer_dense_4 (Dense)        (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                50        \n",
      "=================================================================\n",
      "Total params: 5,637\n",
      "Trainable params: 5,637\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gp_model = create_model(gp_result.x[0],gp_result.x[1],gp_result.x[2],gp_result.x[3],gp_result.x[4],gp_result.x[5])\n",
    "gp_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 5s 86us/step - loss: 0.0107 - acc: 0.9978\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 5s 83us/step - loss: 0.0105 - acc: 0.9978\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 5s 82us/step - loss: 0.0104 - acc: 0.9979\n",
      "10000/10000 [==============================] - 0s 28us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07845962716488976, 0.9809]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#retrain our best model architecture\n",
    "gp_model.fit(X_train,y_train, epochs=3)\n",
    "gp_model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying a gradient boosted search with a simpler model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "tensorflow.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 51000 samples, validate on 9000 samples\n",
      "Epoch 1/3\n",
      "51000/51000 [==============================] - 3s 50us/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.0781 - val_acc: 0.9819\n",
      "Epoch 2/3\n",
      "51000/51000 [==============================] - 2s 45us/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.0782 - val_acc: 0.9821\n",
      "Epoch 3/3\n",
      "51000/51000 [==============================] - 2s 46us/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.0782 - val_acc: 0.9820\n",
      "\n",
      "Accuracy: 98.20%\n",
      "\n",
      "Train on 51000 samples, validate on 9000 samples\n",
      "Epoch 1/3\n",
      "51000/51000 [==============================] - 3s 62us/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.0783 - val_acc: 0.9818\n",
      "Epoch 2/3\n",
      "51000/51000 [==============================] - 3s 58us/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.0784 - val_acc: 0.9818\n",
      "Epoch 3/3\n",
      "51000/51000 [==============================] - 3s 57us/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.0783 - val_acc: 0.9823\n",
      "\n",
      "Accuracy: 98.23%\n",
      "\n",
      "Train on 51000 samples, validate on 9000 samples\n",
      "Epoch 1/3\n",
      "51000/51000 [==============================] - 3s 59us/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.0783 - val_acc: 0.9819\n",
      "Epoch 2/3\n",
      "51000/51000 [==============================] - 3s 56us/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.0783 - val_acc: 0.9818\n",
      "Epoch 3/3\n",
      "51000/51000 [==============================] - 3s 55us/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.0784 - val_acc: 0.9818\n",
      "\n",
      "Accuracy: 98.18%\n",
      "\n",
      "Train on 51000 samples, validate on 9000 samples\n",
      "Epoch 1/3\n",
      "51000/51000 [==============================] - 11s 214us/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.0785 - val_acc: 0.9818\n",
      "Epoch 2/3\n",
      "51000/51000 [==============================] - 11s 209us/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.0783 - val_acc: 0.9821\n",
      "Epoch 3/3\n",
      "51000/51000 [==============================] - 11s 207us/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.0786 - val_acc: 0.9820\n",
      "\n",
      "Accuracy: 98.20%\n",
      "\n",
      "Train on 51000 samples, validate on 9000 samples\n",
      "Epoch 1/3\n",
      "51000/51000 [==============================] - 10s 187us/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.0786 - val_acc: 0.9819\n",
      "Epoch 2/3\n",
      "51000/51000 [==============================] - 9s 180us/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.0785 - val_acc: 0.9821\n",
      "Epoch 3/3\n",
      "51000/51000 [==============================] - 9s 181us/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.0787 - val_acc: 0.9819\n",
      "\n",
      "Accuracy: 98.19%\n",
      "\n",
      "Train on 51000 samples, validate on 9000 samples\n",
      "Epoch 1/3\n",
      "51000/51000 [==============================] - 17s 339us/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.0785 - val_acc: 0.9821\n",
      "Epoch 2/3\n",
      "51000/51000 [==============================] - 17s 337us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.0788 - val_acc: 0.9818\n",
      "Epoch 3/3\n",
      "51000/51000 [==============================] - 17s 332us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.0789 - val_acc: 0.9822\n",
      "\n",
      "Accuracy: 98.22%\n",
      "\n",
      "Train on 51000 samples, validate on 9000 samples\n",
      "Epoch 1/3\n",
      "51000/51000 [==============================] - 2s 38us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.0789 - val_acc: 0.9819\n",
      "Epoch 2/3\n",
      "51000/51000 [==============================] - 2s 35us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.0789 - val_acc: 0.9818\n",
      "Epoch 3/3\n",
      "51000/51000 [==============================] - 2s 35us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.0789 - val_acc: 0.9819\n",
      "\n",
      "Accuracy: 98.19%\n",
      "\n",
      "Train on 51000 samples, validate on 9000 samples\n",
      "Epoch 1/3\n",
      "51000/51000 [==============================] - 2s 44us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.0790 - val_acc: 0.9818\n",
      "Epoch 2/3\n",
      "51000/51000 [==============================] - 2s 40us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.0790 - val_acc: 0.9817\n",
      "Epoch 3/3\n",
      "51000/51000 [==============================] - 2s 41us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.0790 - val_acc: 0.9818\n",
      "\n",
      "Accuracy: 98.18%\n",
      "\n",
      "Train on 51000 samples, validate on 9000 samples\n",
      "Epoch 1/3\n",
      "51000/51000 [==============================] - 6s 110us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.0791 - val_acc: 0.9818\n",
      "Epoch 2/3\n",
      "51000/51000 [==============================] - 5s 105us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.0792 - val_acc: 0.9819\n",
      "Epoch 3/3\n",
      "51000/51000 [==============================] - 5s 105us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.0792 - val_acc: 0.9818\n",
      "\n",
      "Accuracy: 98.18%\n",
      "\n",
      "Train on 51000 samples, validate on 9000 samples\n",
      "Epoch 1/3\n",
      "51000/51000 [==============================] - 2s 36us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.0792 - val_acc: 0.9818\n",
      "Epoch 2/3\n",
      "51000/51000 [==============================] - 2s 34us/step - loss: 0.0010 - acc: 1.0000 - val_loss: 0.0793 - val_acc: 0.9819\n",
      "Epoch 3/3\n",
      "51000/51000 [==============================] - 2s 34us/step - loss: 0.0010 - acc: 1.0000 - val_loss: 0.0792 - val_acc: 0.9818\n",
      "\n",
      "Accuracy: 98.18%\n",
      "\n",
      "Train on 51000 samples, validate on 9000 samples\n",
      "Epoch 1/3\n",
      "51000/51000 [==============================] - 8s 164us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.0792 - val_acc: 0.9819\n",
      "Epoch 2/3\n",
      "51000/51000 [==============================] - 8s 166us/step - loss: 0.0010 - acc: 1.0000 - val_loss: 0.0795 - val_acc: 0.9818\n",
      "Epoch 3/3\n",
      "51000/51000 [==============================] - 8s 155us/step - loss: 0.0010 - acc: 1.0000 - val_loss: 0.0793 - val_acc: 0.9818\n",
      "\n",
      "Accuracy: 98.18%\n",
      "\n",
      "Train on 51000 samples, validate on 9000 samples\n",
      "Epoch 1/3\n",
      "51000/51000 [==============================] - 4s 75us/step - loss: 0.0010 - acc: 1.0000 - val_loss: 0.0794 - val_acc: 0.9818\n",
      "Epoch 2/3\n",
      "51000/51000 [==============================] - 4s 78us/step - loss: 0.0010 - acc: 1.0000 - val_loss: 0.0796 - val_acc: 0.9819\n",
      "Epoch 3/3\n",
      "51000/51000 [==============================] - 4s 71us/step - loss: 0.0010 - acc: 1.0000 - val_loss: 0.0795 - val_acc: 0.9819\n",
      "\n",
      "Accuracy: 98.19%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gbrt_result = gbrt_minimize(func=fitness,\n",
    "                            dimensions=dimensions,\n",
    "                            n_calls=12,\n",
    "                            n_jobs=-1,\n",
    "                            x0=default_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best accuracy was -98.23%.\n"
     ]
    }
   ],
   "source": [
    "print(\"best accuracy was \" + str(round(gbrt_result.fun *100,2))+\"%.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>learning rate</th>\n",
       "      <th>hidden layers</th>\n",
       "      <th>input layer nodes</th>\n",
       "      <th>hidden layer nodes</th>\n",
       "      <th>activation function</th>\n",
       "      <th>batch size</th>\n",
       "      <th>adam learning rate decay</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001000</td>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>13</td>\n",
       "      <td>relu</td>\n",
       "      <td>64</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>98.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.007515</td>\n",
       "      <td>1</td>\n",
       "      <td>395</td>\n",
       "      <td>16</td>\n",
       "      <td>relu</td>\n",
       "      <td>50</td>\n",
       "      <td>0.006402</td>\n",
       "      <td>98.233334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.002703</td>\n",
       "      <td>2</td>\n",
       "      <td>67</td>\n",
       "      <td>18</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>52</td>\n",
       "      <td>0.005823</td>\n",
       "      <td>98.177777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.002827</td>\n",
       "      <td>4</td>\n",
       "      <td>228</td>\n",
       "      <td>6</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>13</td>\n",
       "      <td>0.001937</td>\n",
       "      <td>98.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000681</td>\n",
       "      <td>2</td>\n",
       "      <td>164</td>\n",
       "      <td>22</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>15</td>\n",
       "      <td>0.007204</td>\n",
       "      <td>98.188889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000740</td>\n",
       "      <td>3</td>\n",
       "      <td>224</td>\n",
       "      <td>8</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>8</td>\n",
       "      <td>0.009376</td>\n",
       "      <td>98.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.002231</td>\n",
       "      <td>3</td>\n",
       "      <td>306</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>82</td>\n",
       "      <td>0.001020</td>\n",
       "      <td>98.188889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000307</td>\n",
       "      <td>1</td>\n",
       "      <td>262</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>71</td>\n",
       "      <td>0.002139</td>\n",
       "      <td>98.177777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.001296</td>\n",
       "      <td>2</td>\n",
       "      <td>460</td>\n",
       "      <td>6</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>26</td>\n",
       "      <td>0.009184</td>\n",
       "      <td>98.177777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.005741</td>\n",
       "      <td>5</td>\n",
       "      <td>509</td>\n",
       "      <td>3</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>88</td>\n",
       "      <td>0.003993</td>\n",
       "      <td>98.177778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.001514</td>\n",
       "      <td>4</td>\n",
       "      <td>23</td>\n",
       "      <td>12</td>\n",
       "      <td>relu</td>\n",
       "      <td>17</td>\n",
       "      <td>0.009366</td>\n",
       "      <td>98.177778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.009309</td>\n",
       "      <td>1</td>\n",
       "      <td>366</td>\n",
       "      <td>11</td>\n",
       "      <td>relu</td>\n",
       "      <td>39</td>\n",
       "      <td>0.006318</td>\n",
       "      <td>98.188889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    learning rate  hidden layers  input layer nodes  hidden layer nodes  \\\n",
       "0        0.001000              1                512                  13   \n",
       "1        0.007515              1                395                  16   \n",
       "2        0.002703              2                 67                  18   \n",
       "3        0.002827              4                228                   6   \n",
       "4        0.000681              2                164                  22   \n",
       "5        0.000740              3                224                   8   \n",
       "6        0.002231              3                306                   2   \n",
       "7        0.000307              1                262                   1   \n",
       "8        0.001296              2                460                   6   \n",
       "9        0.005741              5                509                   3   \n",
       "10       0.001514              4                 23                  12   \n",
       "11       0.009309              1                366                  11   \n",
       "\n",
       "   activation function  batch size  adam learning rate decay   accuracy  \n",
       "0                 relu          64                  0.001000  98.200000  \n",
       "1                 relu          50                  0.006402  98.233334  \n",
       "2              sigmoid          52                  0.005823  98.177777  \n",
       "3              sigmoid          13                  0.001937  98.200000  \n",
       "4              sigmoid          15                  0.007204  98.188889  \n",
       "5              sigmoid           8                  0.009376  98.222222  \n",
       "6                 relu          82                  0.001020  98.188889  \n",
       "7              sigmoid          71                  0.002139  98.177777  \n",
       "8              sigmoid          26                  0.009184  98.177777  \n",
       "9              sigmoid          88                  0.003993  98.177778  \n",
       "10                relu          17                  0.009366  98.177778  \n",
       "11                relu          39                  0.006318  98.188889  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([pd.DataFrame(gbrt_result.x_iters, columns = [\"learning rate\",\"hidden layers\",\"input layer nodes\",\"hidden layer nodes\",\n",
    "                                           \"activation function\",\"batch size\",\"adam learning rate decay\"]),\n",
    "(pd.Series(gbrt_result.func_vals*-100, name=\"accuracy\"))], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.007514932641849276, 1, 395, 16, 'relu', 50, 0.0064019416793896135]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbrt_result.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "tensorflow.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 5s 87us/step - loss: 0.0102 - acc: 0.9979\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 5s 84us/step - loss: 0.0100 - acc: 0.9979\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 5s 82us/step - loss: 0.0099 - acc: 0.9979\n",
      "10000/10000 [==============================] - 0s 25us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07832179739567255, 0.9813]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#call our best model \n",
    "gbrt_model = create_model(gbrt_result.x[0],gbrt_result.x[1],gbrt_result.x[2],gbrt_result.x[3],gbrt_result.x[4],gbrt_result.x[5])\n",
    "gbrt_model.summary()\n",
    "#retrain our best model architecture\n",
    "model.fit(X_train,y_train, epochs=3)\n",
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## While the two models had very different parameters, their accuracies were basically the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "much thanks to this link for providing a framework for this code.\n",
    "https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/19_Hyper-Parameters.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
